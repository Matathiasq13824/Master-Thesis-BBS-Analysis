{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter notebook used to train the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "from typing import List, Tuple\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function loading the proofs and separating the labels and feature for the training\n",
    "def load_data(features_kept, file_name, n_rows = None):\n",
    "    features_data = (pd.read_csv(file_name, nrows=n_rows)\n",
    "    .filter(regex=features_kept)\n",
    "    .to_numpy().transpose())\n",
    "    \n",
    "    features = features_data[1:].transpose()\n",
    "    labels = features_data[0]\n",
    "    del features_data\n",
    "\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function who train model based on the label and the feature given as input, and evaluate them.\n",
    "def classify(train_features, train_labels, test_features, test_labels, name_model, type_model):\n",
    "    # Select the type of model\n",
    "    if type_model == \"hist\":\n",
    "        clf = HistGradientBoostingClassifier(verbose=2)\n",
    "    elif type_model == \"neuronal\":\n",
    "        clf = MLPClassifier(hidden_layer_sizes=(1000,100),verbose= True)\n",
    "    else:\n",
    "        clf = RandomForestClassifier(verbose= 2, n_jobs=-1, max_depth = 100)\n",
    "\n",
    "    # Train the classifier using the training features and labels.\n",
    "    clf.fit(train_features, train_labels)\n",
    "    # Use the classifier to make predictions on the test features.\n",
    "    predictions = clf.predict(test_features)\n",
    "    proba = clf.predict_proba(test_features)\n",
    "    # clf do already some of the work for us\n",
    "    score = clf.score(test_features, test_labels)\n",
    "    # Store the model if the need of a retraining or a re-evaluation is needed.\n",
    "    joblib.dump(clf, name_model, compress=9)\n",
    "\n",
    "    \n",
    "    return predictions, proba, score, clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_crossval(features, labels, run, folds=10, model= \"hist\", regex = \"\", name_proofs = \"\", output_folder = \"\"):\n",
    "    labels = np.array(labels)\n",
    "    features = np.array(features)\n",
    "\n",
    "    # dataFrame used to store the evaluation of the models for future analysis\n",
    "    df = pd.DataFrame(columns= [\n",
    "        \"name_model\",\n",
    "        \"type_model\",\n",
    "        \"regex\",\n",
    "        \"trial_name\",\n",
    "        \"params_model\",\n",
    "        \"proofs_name\",\n",
    "        \"total_number_proofs\",\n",
    "        \"len_test_index\",\n",
    "        \"test_index\",\n",
    "        \"test_labels\",\n",
    "        \"predictions\",\n",
    "        # \"proba\", # Too large to save\n",
    "        \"score\",\n",
    "        \"score2\",\n",
    "        \"score10\",\n",
    "    ])\n",
    "\n",
    "    total_top_2_accuracy = 0.0\n",
    "    total_top_10_accuracy = 0.0\n",
    "    total_score = 0.0\n",
    "    # StratifiedKFold will generate n_splits times a random training and evaluation subset from the data,\n",
    "    # each of them containing the same proportion of different lables than the total set\n",
    "    kf = StratifiedKFold(n_splits=folds)\n",
    "    for idx, indexes in enumerate(kf.split(features, labels)):\n",
    "        print(\"==== New Fold ====\")\n",
    "        name_model = f\"model_{idx}_run{run}\"\n",
    "        train_index, test_index = indexes\n",
    "        X_train, X_test = features[train_index], features[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        predictions, proba, score, model_full = classify(X_train, y_train, X_test, y_test, f\"{output_folder}/{name_model}\", model)\n",
    "        total_score += score\n",
    "        score2 = calculate_accuracy(proba, y_test, 2)\n",
    "        score10 = calculate_accuracy(proba, y_test, 10)\n",
    "        total_top_2_accuracy += score2\n",
    "        total_top_10_accuracy += score10\n",
    "        # Store the evaluation of the newly trained model\n",
    "        df = pd.concat([pd.DataFrame([[\n",
    "            name_model, #done\n",
    "            str(type(model_full)), #done\n",
    "            regex,\n",
    "            output_folder,\n",
    "            str(model_full.get_params()), #done\n",
    "            name_proofs,\n",
    "            str(len(labels)), #done\n",
    "            str(len(test_index)), #done\n",
    "            str(test_index.tolist()), #done\n",
    "            str(y_test.tolist()), #done\n",
    "            str(predictions.tolist()), #done\n",
    "            # str(proba.tolist()),\n",
    "            str(score), #done\n",
    "            str(score2), #done\n",
    "            str(score10), #done\n",
    "        ]], columns=df.columns), df], ignore_index=True)\n",
    "        del model_full\n",
    "    df.to_csv(f\"{output_folder}/model_run{run}_simplified.csv\")\n",
    "        \n",
    "    print(\"Total aveage prediction rate: {}\".format(total_score / folds))\n",
    "    print(\"Average prediction rate for top 2: {}\".format(total_top_2_accuracy / folds))\n",
    "    print(\"Average prediction rate for top 10: {}\".format(total_top_10_accuracy / folds))\n",
    "\n",
    "# Functions used to calculate score2 and score10\n",
    "\n",
    "def sort_predictions(predictions_proba: NDArray, y_test: NDArray) -> List[Tuple[int, List[Tuple[int, float]]]]:\n",
    "    ordered_predictions = []\n",
    "    for y, pred_prob in zip(y_test, predictions_proba):\n",
    "        sorted_proba = sorted([(i + 1, prob) for i, prob in enumerate(pred_prob) if prob > 0], key=lambda x: x[1], reverse=True)\n",
    "        ordered_predictions.append((y, sorted_proba))\n",
    "    return ordered_predictions\n",
    "\n",
    "def calculate_accuracy(predictions_proba: NDArray, y_test: NDArray, N: int = 100) -> float:\n",
    "    sorted_predictions = sort_predictions(predictions_proba, y_test)\n",
    "    nb_IN_N_tops = sum(y in [pred[0] for pred in sorted_pred[:N]] for y, sorted_pred in sorted_predictions)\n",
    "    return float(nb_IN_N_tops) / len(predictions_proba)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to reevaluate models if necessary\n",
    "def perform_crossval_evaluation(features, labels, folder_model, run, folds=10, regex = \"\", name_proofs = \"\"):\n",
    "    labels = np.array(labels)\n",
    "    features = np.array(features)\n",
    "\n",
    "    df = pd.DataFrame(columns= [\n",
    "        \"name_model\",\n",
    "        \"type_model\",\n",
    "        \"regex\",\n",
    "        \"trial_name\",\n",
    "        \"params_model\",\n",
    "        \"proofs_name\",\n",
    "        \"total_number_proofs\",\n",
    "        \"len_test_index\",\n",
    "        \"test_index\",\n",
    "        \"test_labels\",\n",
    "        \"predictions\",\n",
    "        # \"proba\", # Too large to save\n",
    "        \"score\",\n",
    "        \"score2\",\n",
    "        \"score10\",\n",
    "    ])\n",
    "\n",
    "    total_top_2_accuracy = 0.0\n",
    "    total_top_10_accuracy = 0.0\n",
    "    total_score = 0.0\n",
    "    kf = StratifiedKFold(n_splits=folds)\n",
    "    for idx, indexes in enumerate(kf.split(features, labels)):\n",
    "        print(\"==== New Fold ====\")\n",
    "        _, test_index = indexes\n",
    "        X_test =  features[test_index]\n",
    "        y_test = labels[test_index]\n",
    "        model = joblib.load(f\"{folder_model}/model_{idx}_run{run}_half\")\n",
    "        print(\"model loaded\")\n",
    "        predictions = model.predict(X_test)\n",
    "        proba = model.predict_proba(X_test)\n",
    "        score = model.score(X_test, y_test)\n",
    "        total_score += score\n",
    "        score2 = calculate_accuracy(proba, y_test, 2)\n",
    "        score10 = calculate_accuracy(proba, y_test, 10)\n",
    "        total_top_2_accuracy += score2\n",
    "        total_top_10_accuracy += score10\n",
    "        df = pd.concat([pd.DataFrame([[\n",
    "            f\"model_{idx}_run{run}\", #done\n",
    "            str(type(model)), #done\n",
    "            regex,\n",
    "            folder_model,\n",
    "            str(model.get_params()), #done\n",
    "            name_proofs,\n",
    "            str(len(labels)), #done\n",
    "            str(len(test_index)), #done\n",
    "            str(test_index.tolist()), #done\n",
    "            str(y_test.tolist()), #done\n",
    "            str(predictions.tolist()), #done\n",
    "            # str(proba.tolist()),\n",
    "            str(score), #done\n",
    "            str(score2), #done\n",
    "            str(score10), #done\n",
    "        ]], columns=df.columns), df], ignore_index=True)\n",
    "        del model\n",
    "    df.to_csv(f\"{folder_model}/model_run{run}_simplified.csv\")\n",
    "        \n",
    "    print(\"Total aveage prediction rate: {}\".format(total_score / folds))\n",
    "    print(\"Average prediction rate for top 2: {}\".format(total_top_2_accuracy / folds))\n",
    "    print(\"Average prediction rate for top 10: {}\".format(total_top_10_accuracy / folds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple of training of models.\n",
    "regex = \"Personal Number|number_attributes|Size|proof_array_\\d+\"\n",
    "name_training_data = \"formatted_run.csv\"\n",
    "folder_models = \"models_run\"\n",
    "# Trying to train models with more than 500'000 proofs have lead to crash or runs of multiple days, \n",
    "# thus 500k is the maximum recommended for the training. \n",
    "n_rows = 10000\n",
    "features, labels = load_data(regex, name_training_data, n_rows) \n",
    "perform_crossval(features, labels, 1, folds=10, model= \"hist\", regex = regex, name_proofs=name_training_data, output_folder=folder_models)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
